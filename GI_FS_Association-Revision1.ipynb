{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\"> Import libraries<span style=\"color:green\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 09:53:20.860974: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-20 09:53:20.861052: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-20 09:53:20.862438: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-20 09:53:20.871958: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-20 09:53:21.754965: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#Import all the required libraries\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import random\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, ReLU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import pybiomart\n",
    "from biomart import BiomartServer\n",
    "from sklearn.feature_selection import RFECV\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\"> ML for Feature Selection and Association <span style=\"color:green\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     FID  IID  PAT  MAT  SEX  PHENOTYPE  rs2980319_T  rs6685064_C  \\\n",
      "0  10004    1    0    0    2         75            2            1   \n",
      "1  10005    1    0    0    1         69            2            2   \n",
      "2  10007    1    0    0    1        108            2            2   \n",
      "3  10008    1    0    0    1         94            2            2   \n",
      "4  10009    1    0    0    1         92            2            2   \n",
      "\n",
      "   rs2281173_G  rs10907187_G  ...  rs17001322_G  rs4141437_G  rs1033665_A  \\\n",
      "0            1             2  ...             2            2            2   \n",
      "1            2             2  ...             2            2            2   \n",
      "2            2             0  ...             2            2            2   \n",
      "3            2             2  ...             2            2            2   \n",
      "4            2             2  ...             2            2            2   \n",
      "\n",
      "   rs5771133_T  rs3810974_C  rs138228_A  rs9617081_A  rs12484907_G  \\\n",
      "0            2            2           0            1             2   \n",
      "1            2            2           0            2             2   \n",
      "2            2            2           2            2             2   \n",
      "3            2            2           0            0             2   \n",
      "4            1            2           1            2             2   \n",
      "\n",
      "   rs17248301_T  rs8139476_C  \n",
      "0             2            2  \n",
      "1             2            2  \n",
      "2             2            2  \n",
      "3             2            1  \n",
      "4             2            1  \n",
      "\n",
      "[5 rows x 69908 columns]\n",
      "Shape of file (1282, 69908)\n"
     ]
    }
   ],
   "source": [
    "#Read the file \n",
    "df= pd.read_table(\"/home/Vaishnavi/ldlGWAS/GWAStutorialldlraw.raw\", delim_whitespace= True)\n",
    "print(df.head())\n",
    "print(\"Shape of file\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FID', 'IID', 'PAT', 'MAT', 'SEX', 'PHENOTYPE', 'rs2980319',\n",
       "       'rs6685064', 'rs2281173', 'rs10907187',\n",
       "       ...\n",
       "       'rs17001322', 'rs4141437', 'rs1033665', 'rs5771133', 'rs3810974',\n",
       "       'rs138228', 'rs9617081', 'rs12484907', 'rs17248301', 'rs8139476'],\n",
       "      dtype='object', length=69908)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_col_names= list(df.columns[:6])\n",
    "new_col_names.extend([col_name[:-2] for col_name in df.columns[6:]])\n",
    "df.columns=new_col_names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (1282, 69902)\n",
      "Shape of y (1282,)\n"
     ]
    }
   ],
   "source": [
    "#X is Independent feature and y is dependent feature\n",
    "X= df.iloc[:, 6:]\n",
    "print(\"Shape of X\", X.shape)\n",
    "y= df.PHENOTYPE\n",
    "print(\"Shape of y\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of X_train is (897, 69902)\n",
      "The dimension of X_test is (385, 69902)\n"
     ]
    }
   ],
   "source": [
    "#Train test data split\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=2000)\n",
    "print(\"The dimension of X_train is {}\".format(X_train.shape))\n",
    "print(\"The dimension of X_test is {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> LASSO <span style=\"color:green\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/compiler/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.377e+02, tolerance: 1.336e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/apps/compiler/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.548e+02, tolerance: 1.342e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/apps/compiler/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.105e+02, tolerance: 1.252e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/apps/compiler/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.230e+02, tolerance: 1.270e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/apps/compiler/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.026e+02, tolerance: 1.218e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 0.00045\n",
      "5003 selected SNPs\n",
      "print shape of selected snps by lasso (1282, 5003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/compiler/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.499e+02, tolerance: 1.605e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the LASSOCV model select 5000 SNP\n",
    "lasso_cv = linear_model.LassoCV(alphas= [0.00045], cv=5, random_state=2000)\n",
    "\n",
    "# Fit the model to the data\n",
    "lasso_cv.fit(X,y)\n",
    "\n",
    "#Print the optimal alpha value\n",
    "print (\"Optimal alpha:\", lasso_cv.alpha_)\n",
    "\n",
    "# Indices of the SNPs with non-zero coefficients\n",
    "lasso_snps_ldl = np.where(lasso_cv.coef_)[0]\n",
    "\n",
    "# See how many SNPs have a non-zero coefficient\n",
    "print(len(lasso_snps_ldl), \"selected SNPs\")\n",
    "\n",
    "#Final SNPs\n",
    "l1= X.iloc[:,lasso_snps_ldl]\n",
    "print(\"print shape of selected snps by lasso\",l1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape (1282,)\n",
      "x_train=(897, 5003), x_test=(385, 5003), Y_train= (897,), Y_test= (385,)\n",
      "r2 is  0.6357759780110592\n",
      "mean_absolute_error 16.056727661782173\n",
      "mean_squared_error  391.95561760695495\n",
      "explained_variance_score 0.6366066443871343\n",
      "Selected F-values:\n",
      "Indices of selected features with top F-values:\n",
      "Top selected features:\n"
     ]
    }
   ],
   "source": [
    "Y= df.PHENOTYPE.values\n",
    "print(\"Y.shape\",Y.shape)\n",
    "x_train, x_test, Y_train, Y_test= train_test_split(l1, Y, test_size=0.3, random_state=2000)\n",
    "print(f\"x_train={x_train.shape}, x_test={x_test.shape}, Y_train= {Y_train.shape}, Y_test= {Y_test.shape}\")\n",
    "scaler= StandardScaler()\n",
    "x_train= scaler.fit_transform(x_train)\n",
    "x_test= scaler.transform(x_test)\n",
    "\n",
    "random.seed(2000)\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(x_train, Y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "score= r2_score(Y_test, y_pred)\n",
    "print(\"r2 is \", score)\n",
    "h= mean_absolute_error(Y_test, y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(y_pred, Y_test))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))\n",
    "\n",
    "f_statistic, p_value = f_regression(l1, Y)\n",
    "selected_p_indices = np.where(p_value < 5e-04)[0]\n",
    "selected_p_values = p_value[selected_p_indices]\n",
    "#print(\"Selected p-values:\")\n",
    "#print(selected_p_values)\n",
    "#print(\"Number of selected features:\", len(selected_p_indices))\n",
    "selected_f_values = f_statistic[selected_p_indices]\n",
    "print(\"Selected F-values:\")\n",
    "#print(selected_f_values)\n",
    "top_f_indices = np.argsort(-selected_f_values)\n",
    "print(\"Indices of selected features with top F-values:\")\n",
    "#print(top_f_indices)\n",
    "top_f_features = l1.iloc[:, selected_p_indices[top_f_indices]]\n",
    "print(\"Top selected features:\")\n",
    "#print(top_f_features)\n",
    "lasso_LR_sigSNP=l1.iloc[:, selected_p_indices]\n",
    "#lasso_LR_sigSNP\n",
    "lasso_LR_sig_col= list(lasso_LR_sigSNP.columns)\n",
    "#lasso_LR_sig_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features indices: [  13   14   19   43   64   70   90   91   98  122  140  169  195  199\n",
      "  207  224  238  245  262  263  278  279  297  305  344  352  365  438\n",
      "  451  453  469  490  519  532  553  560  595  623  635  640  646  652\n",
      "  681  686  692  749  791  794  805  864  867  870  872  888  896  902\n",
      "  913  928  931  940  951  975  985 1025 1028 1029 1032 1035 1039 1053\n",
      " 1060 1095 1120 1127 1136 1139 1155 1169 1175 1188 1191 1195 1202 1215\n",
      " 1218 1239 1241 1258 1273 1274 1281 1290 1293 1307 1319 1324 1370 1375\n",
      " 1379 1389]\n",
      "r2 is  0.020866559486800607\n",
      "mean_absolute_error 26.32997402597403\n",
      "mean_squared_error  1053.6835277922078\n",
      "explained_variance_score 0.021994776157934992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rs12085743',\n",
       " 'rs11587314',\n",
       " 'rs12142584',\n",
       " 'rs12078796',\n",
       " 'rs1193219',\n",
       " 'rs161807',\n",
       " 'rs12125492',\n",
       " 'rs4846216',\n",
       " 'rs2379152',\n",
       " 'rs12758763',\n",
       " 'rs2235918',\n",
       " 'rs2743208',\n",
       " 'rs230175',\n",
       " 'rs1043424',\n",
       " 'rs3738091',\n",
       " 'rs1555024',\n",
       " 'rs4649033',\n",
       " 'rs4659362',\n",
       " 'rs9426278',\n",
       " 'rs9286959',\n",
       " 'rs592232',\n",
       " 'rs728340',\n",
       " 'rs7521293',\n",
       " 'rs10798977',\n",
       " 'rs1110113',\n",
       " 'rs2484754',\n",
       " 'rs12563174',\n",
       " 'rs17108782',\n",
       " 'rs9332417',\n",
       " 'rs744748',\n",
       " 'rs4278395',\n",
       " 'rs1202824',\n",
       " 'rs6676078',\n",
       " 'rs2782549',\n",
       " 'rs17318954',\n",
       " 'rs11208264',\n",
       " 'rs2863200',\n",
       " 'rs535112',\n",
       " 'rs2422137',\n",
       " 'rs1405058',\n",
       " 'rs7512474',\n",
       " 'rs211764',\n",
       " 'rs12125903',\n",
       " 'rs473027',\n",
       " 'rs11162662',\n",
       " 'rs569399',\n",
       " 'rs284152',\n",
       " 'rs10874755',\n",
       " 'rs1687949',\n",
       " 'rs12023867',\n",
       " 'rs10493973',\n",
       " 'rs6658728',\n",
       " 'rs2229783',\n",
       " 'rs11581564',\n",
       " 'rs2494067',\n",
       " 'rs10494100',\n",
       " 'rs3768480',\n",
       " 'rs2786152',\n",
       " 'rs1264897',\n",
       " 'rs416269',\n",
       " 'rs4838986',\n",
       " 'rs7529979',\n",
       " 'rs1146316',\n",
       " 'rs4079531',\n",
       " 'rs2335407',\n",
       " 'rs759330',\n",
       " 'rs11264640',\n",
       " 'rs6694377',\n",
       " 'rs6699950',\n",
       " 'rs12726858',\n",
       " 'rs7529215',\n",
       " 'rs12034420',\n",
       " 'rs7545852',\n",
       " 'rs2093658',\n",
       " 'rs17485889',\n",
       " 'rs2255438',\n",
       " 'rs1547282',\n",
       " 'rs3850640',\n",
       " 'rs898658',\n",
       " 'rs7521851',\n",
       " 'rs6425430',\n",
       " 'rs12074053',\n",
       " 'rs12097041',\n",
       " 'rs596424',\n",
       " 'rs10911021',\n",
       " 'rs330743',\n",
       " 'rs2495474',\n",
       " 'rs4507990',\n",
       " 'rs12126563',\n",
       " 'rs4657960',\n",
       " 'rs1408868',\n",
       " 'rs12048004',\n",
       " 'rs11579522',\n",
       " 'rs17459837',\n",
       " 'rs10800813',\n",
       " 'rs1418444',\n",
       " 'rs12119793',\n",
       " 'rs12035745',\n",
       " 'rs10863776',\n",
       " 'rs182743']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Random Forest Regressor\n",
    " #reate a Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=9, min_samples_leaf=4, min_samples_split=2, \n",
    "                                     n_jobs=70,random_state=2000)\n",
    "\n",
    "# Create RFECV object\n",
    "rfecv = RFECV(estimator=rf_regressor, step=30, cv=5, scoring='neg_mean_squared_error', n_jobs=70)\n",
    "\n",
    "# Fit RFECV on the training data\n",
    "rfecv.fit(x_train, Y_train)\n",
    "\n",
    "# Manually set the number of features you want to keep\n",
    "desired_num_features =100\n",
    "# Manually set the number of features you want to keep\n",
    "\n",
    "\n",
    "# Get the selected features based on the desired number\n",
    "selected_features = rfecv.get_support(indices=True)[:desired_num_features]\n",
    "# Print the indices of the selected features\n",
    "print(\"Selected features indices:\", selected_features)\n",
    "\n",
    "# Transform the training and testing data to keep only the selected features\n",
    "X_train_selected = rfecv.transform(x_train)\n",
    "X_test_selected = rfecv.transform(x_test)\n",
    "\n",
    "\n",
    "# Get the optimal number of features\n",
    "optimal_num_features = rfecv.n_features_\n",
    "\n",
    "# Print the optimal number of features\n",
    "#print(f\"Optimal number of features: {optimal_num_features}\")\n",
    "\n",
    "\n",
    "# Train a Random Forest Regressor on the selected features\n",
    "rf_regressor_selected = RandomForestRegressor(n_estimators=100, random_state=2000)\n",
    "rf_regressor_selected.fit(X_train_selected, Y_train)\n",
    "\n",
    "# Make predictions on the test set using the model with selected features\n",
    "y_pred= rf_regressor_selected.predict(X_test_selected)\n",
    "score= r2_score(Y_test, y_pred)\n",
    "print(\"r2 is \", score)\n",
    "h= mean_absolute_error(Y_test, y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(y_pred, Y_test))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))\n",
    "lasso_RF_sigSNP=l1.iloc[:,selected_features]\n",
    "lasso_RF_sig_col= list(lasso_RF_sigSNP.columns)\n",
    "lasso_RF_sig_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 is  0.657599345539942\n",
      "mean_absolute_error 15.4160111992304\n",
      "mean_squared_error  368.4706441246004\n",
      "explained_variance_score 0.6590080478335235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "svr = SVR(kernel = 'sigmoid' , C=100, epsilon= 0.01, gamma= 'scale', cache_size=800)\n",
    "svr.fit(x_train, Y_train)\n",
    "y_pred= svr.predict(x_test)\n",
    "score= r2_score(Y_test, y_pred)\n",
    "print(\"r2 is \", score)\n",
    "h= mean_absolute_error(Y_test, y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(y_pred, Y_test))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))\n",
    "\n",
    "#perm_importance= permutation_importance(svr, x_test, Y_test, n_repeats=10, random_state=2000, n_jobs=70)\n",
    "\n",
    "#imp_score=perm_importance.importances_mean\n",
    "#feature_indices = np.argsort(imp_score)[::-1]\n",
    "#print(f\"Selected feature indices are {feature_indices}\")\n",
    "\n",
    "# # Specify the number of features you want to select based on magnitude\n",
    "#num_selected_features = 100\n",
    "#top_100=feature_indices[:num_selected_features]\n",
    "#print(f\"Top 100 fearure indices are {feature_indices}\")\n",
    "#imp_score_100=imp_score[top_100]\n",
    "#print(f\"Top 100 fearures permutation scores are {imp_score_100}\")\n",
    "#lasso_SVR_sigSNP= l1.iloc[:,top_100]\n",
    "#lasso_SVR_sig_col= list(lasso_SVR_sigSNP.columns)\n",
    "#lasso_SVR_sig_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score -0.045758213198292985\n",
      "mean_absolute_error 26.94532488538073\n",
      "mean_squared_error  1125.3810335830306\n",
      "explained_variance_score -0.04165763446728543\n"
     ]
    }
   ],
   "source": [
    "regressor=xgb.XGBRegressor(learning_rate= 0.01, n_estimators=900,  max_depth=10, eval_metric='rmsle', random_state=2000)\n",
    "regressor.fit(x_train, Y_train)\n",
    "y_pred= regressor.predict(x_test)\n",
    "score= r2_score(Y_test,y_pred)\n",
    "print(\"r2 score\",score)\n",
    "h= mean_absolute_error(Y_test,y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> Ridge<span style=\"color:green\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 0.005\n",
      "Selected SNPs indices based on top coefficients: [67847  5994 16614 ... 10679 20908 16917]\n",
      "5000 selected SNPs\n",
      "Maximum value of coefficient 0.17897713157981693\n",
      "print shape of selected snps by Ridge (1282, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the RidgeCV model select 5000 SNP\n",
    "ridge_cv = linear_model.RidgeCV(alphas=[0.005], cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_cv.fit(X, y)\n",
    "\n",
    "# Optimal alpha value\n",
    "print(\"Optimal alpha:\", ridge_cv.alpha_)\n",
    "\n",
    "# Indices of the SNPs with non-zero coefficients\n",
    "ridge_snps_ldl = np.where(ridge_cv.coef_)[0]\n",
    "\n",
    "# Select coefficients with non-zero values\n",
    "selected_features = ridge_cv.coef_ != 0\n",
    "\n",
    "# Select coefficients of selected SNPs\n",
    "coefficients = ridge_cv.coef_[selected_features]\n",
    "\n",
    "# Get the indices of the top 5000 SNPs based on descending coefficients\n",
    "top_n = 5000\n",
    "sorted_indices = np.argsort(-coefficients)\n",
    "selected_indices = sorted_indices[:top_n]\n",
    "\n",
    "# Print the indices of selected SNPs and the number of selected SNPs\n",
    "print(\"Selected SNPs indices based on top coefficients:\", selected_indices)\n",
    "print(len(selected_indices), \"selected SNPs\")\n",
    "\n",
    "#Maximum Coefficient\n",
    "print(\"Maximum value of coefficient\", coefficients.max())\n",
    "\n",
    "#Final SNPs\n",
    "R1= X.iloc[:,selected_indices]\n",
    "print(\"print shape of selected snps by Ridge\",R1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape (1282,)\n",
      "x_train=(897, 5000), x_test=(385, 5000), Y_train= (897,), Y_test= (385,)\n"
     ]
    }
   ],
   "source": [
    "Y= df.PHENOTYPE.values\n",
    "print(\"Y.shape\",Y.shape)\n",
    "x_train, x_test, Y_train, Y_test= train_test_split(R1, Y, test_size=0.3, random_state=2000)\n",
    "print(f\"x_train={x_train.shape}, x_test={x_test.shape}, Y_train= {Y_train.shape}, Y_test= {Y_test.shape}\")\n",
    "scaler= StandardScaler()\n",
    "x_train= scaler.fit_transform(x_train)\n",
    "x_test= scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 is  0.7941121665413645\n",
      "mean_absolute_error 11.576119452686955\n",
      "mean_squared_error  221.56389488084804\n",
      "explained_variance_score 0.7941732648183302\n",
      "Selected p-values:\n"
     ]
    }
   ],
   "source": [
    "random.seed(2000)\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(x_train, Y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "score= r2_score(Y_test, y_pred)\n",
    "print(\"r2 is \", score)\n",
    "h= mean_absolute_error(Y_test, y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))\n",
    "\n",
    "#f_statistic, p_value = f_regression(R1, Y)\n",
    "#selected_p_indices = np.where(p_value < 5e-04)[0]\n",
    "#selected_p_values = p_value[selected_p_indices]\n",
    "print(\"Selected p-values:\")\n",
    "#print(selected_p_values)\n",
    "#print(\"Number of selected features:\", len(selected_p_indices))\n",
    "#selected_f_values = f_statistic[selected_p_indices]\n",
    "#print(\"Selected F-values:\")\n",
    "#print(selected_f_values)\n",
    "#top_f_indices = np.argsort(-selected_f_values)\n",
    "#print(\"Indices of selected features with top F-values:\")\n",
    "#print(top_f_indices)\n",
    "#top_f_features = R1.iloc[:, selected_p_indices[top_f_indices]]\n",
    "#print(\"Top selected features:\")\n",
    "#print(top_f_features)\n",
    "#Ridge_LR_sigSNP=R1.iloc[:, selected_p_indices]\n",
    "#Ridge_LR_sigSNP\n",
    "#Ridge_LR_sig_col= list(Ridge_LR_sigSNP.columns)\n",
    "#Ridge_LR_sig_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features indices: [  0   1   2   3   5   6   7   9  11  13  14  15  16  17  18  19  20  21\n",
      "  22  25  27  28  29  30  31  32  34  35  36  37  38  40  42  43  44  46\n",
      "  47  48  51  52  54  58  59  61  63  64  65  66  68  71  74  75  77  78\n",
      "  79  81  82  83  84  85  86  93  94  95 100 105 107 110 111 112 113 115\n",
      " 116 118 120 122 123 124 125 126 128 130 133 134 135 137 138 140 141 142\n",
      " 143 144 147 149 151 152 153 154 155 156]\n",
      "r2 is  0.04247914460022373\n",
      "mean_absolute_error 25.972415584415582\n",
      "mean_squared_error  1030.4253854545454\n",
      "explained_variance_score 0.05089197875277074\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=9, min_samples_leaf=4, min_samples_split=2, \n",
    "                                     n_jobs=10,random_state=2000)\n",
    "\n",
    "# Create RFECV object\n",
    "rfecv = RFECV(estimator=rf_regressor, step=30, cv=5, scoring='neg_mean_squared_error', n_jobs=4)\n",
    "\n",
    "# Fit RFECV on the training data\n",
    "rfecv.fit(x_train, Y_train)\n",
    "\n",
    "# Manually set the number of features you want to keep\n",
    "desired_num_features =100\n",
    "# Manually set the number of features you want to keep\n",
    "\n",
    "\n",
    "# Get the selected features based on the desired number\n",
    "selected_features = rfecv.get_support(indices=True)[:desired_num_features]\n",
    "# Print the indices of the selected features\n",
    "print(\"Selected features indices:\", selected_features)\n",
    "\n",
    "# Transform the training and testing data to keep only the selected features\n",
    "X_train_selected = rfecv.transform(x_train)[:, selected_features]\n",
    "X_test_selected = rfecv.transform(x_test)[:, selected_features]\n",
    "\n",
    "\n",
    "# Get the optimal number of features\n",
    "optimal_num_features = rfecv.n_features_\n",
    "\n",
    "# Print the optimal number of features\n",
    "#print(f\"Optimal number of features: {optimal_num_features}\")\n",
    "\n",
    "\n",
    "# Train a Random Forest Regressor on the selected features\n",
    "rf_regressor_selected = RandomForestRegressor(n_estimators=100, random_state=2000)\n",
    "rf_regressor_selected.fit(X_train_selected, Y_train)\n",
    "\n",
    "# Make predictions on the test set using the model with selected features\n",
    "y_pred = rf_regressor_selected.predict(X_test_selected)\n",
    "\n",
    "score= r2_score(Y_test, y_pred)\n",
    "print(\"r2 is \", score)\n",
    "h= mean_absolute_error(Y_test, y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(y_pred, Y_test))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))\n",
    "#Ridge_RF_sigSNP=R1.iloc[:,selected_features]\n",
    "#Ridge_RF_sig_col= list(Ridge_RF_sigSNP.columns)\n",
    "#Ridge_RF_sig_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 is  0.7943502516488743\n",
      "mean_absolute_error 11.597831965819362\n",
      "mean_squared_error  221.3076822487228\n",
      "explained_variance_score 0.7943648189631427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "svr = SVR(kernel = 'sigmoid' , C=100, epsilon= 0.01, gamma= 'scale', cache_size=800)\n",
    "svr.fit(x_train, Y_train)\n",
    "y_pred= svr.predict(x_test)\n",
    "score= r2_score(Y_test, y_pred)\n",
    "print(\"r2 is \", score)\n",
    "h= mean_absolute_error(Y_test, y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(y_pred, Y_test))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))\n",
    "\n",
    "#perm_importance= permutation_importance(svr, x_test, Y_test, n_repeats=10, random_state=2000, n_jobs=70)\n",
    "\n",
    "#imp_score=perm_importance.importances_mean\n",
    "#feature_indices = np.argsort(np.abs(imp_score))[::-1]\n",
    "#print(f\"Selected feature indices are {feature_indices}\")\n",
    "\n",
    "# # Specify the number of features you want to select based on magnitude\n",
    "#num_selected_features = 100\n",
    "#top_100=feature_indices[:num_selected_features]\n",
    "#print(f\"Top 100 fearure indices are {feature_indices}\")\n",
    "#imp_score_100=imp_score[top_100]\n",
    "#print(f\"Top 100 fearures permutation scores are {imp_score_100}\")\n",
    "\n",
    "#Ridge_SVR_sigSNP= R1.iloc[:,top_100]\n",
    "#Ridge_SVR_sig_col= list(Ridge_SVR_sigSNP.columns)\n",
    "#Ridge_SVR_sig_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score -0.37991823914097145\n",
      "mean_absolute_error 28.730934380865715\n",
      "mean_squared_error  1484.9836172695495\n",
      "explained_variance_score -0.3699044458112726\n"
     ]
    }
   ],
   "source": [
    "regressor=xgb.XGBRegressor(learning_rate= 0.01, n_estimators=900,  max_depth=10, eval_metric='rmsle', random_state=2000)\n",
    "regressor.fit(x_train, Y_train)\n",
    "y_pred= regressor.predict(x_test)\n",
    "score= r2_score(Y_test,y_pred)\n",
    "print(\"r2 score\",score)\n",
    "h= mean_absolute_error(Y_test,y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> Elastic net<span style=\"color:green\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/compiler/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.977e+02, tolerance: 1.336e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/apps/compiler/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.556e+02, tolerance: 1.342e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/apps/compiler/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.812e+02, tolerance: 1.252e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/apps/compiler/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.092e+02, tolerance: 1.270e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/apps/compiler/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.623e+02, tolerance: 1.218e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 0.0033\n",
      "Optimal l1_ratio:  0.5\n",
      "5037 selected SNPs\n",
      "print shape of selected snps by Elastic net (1282, 5037)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/compiler/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.031e+02, tolerance: 1.605e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the Elasticnet model\n",
    "elastic_cv = linear_model.ElasticNetCV(l1_ratio=[0.5],alphas=[0.0033], cv=5, random_state= 2000)\n",
    "elastic_cv.fit(X, y)\n",
    "\n",
    "# Print the optimal alpha value\n",
    "print (\"Optimal alpha:\", elastic_cv.alpha_)\n",
    "print(\"Optimal l1_ratio: \", elastic_cv.l1_ratio_)\n",
    "\n",
    "# Indices of the SNPs with non-zero coefficients\n",
    "elastic_snps_ldl = np.where(elastic_cv.coef_)[0]\n",
    "\n",
    "# See how many SNPs have a non-zero coefficient\n",
    "print(len(elastic_snps_ldl), \"selected SNPs\")\n",
    "\n",
    "#Final SNPs\n",
    "E1= X.iloc[:,elastic_snps_ldl]\n",
    "print(\"print shape of selected snps by Elastic net\",E1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=list(E1.columns)[:5038]\n",
    "#E1.to_(\"/home/Elasticnet_SVR_5000_csv\", index=0)\n",
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape (1282,)\n",
      "x_train=(897, 5037), x_test=(385, 5037), Y_train= (897,), Y_test= (385,)\n"
     ]
    }
   ],
   "source": [
    "Y= df.PHENOTYPE.values\n",
    "print(\"Y.shape\",Y.shape)\n",
    "x_train, x_test, Y_train, Y_test= train_test_split(E1, Y, test_size=0.3, random_state=2000)\n",
    "print(f\"x_train={x_train.shape}, x_test={x_test.shape}, Y_train= {Y_train.shape}, Y_test= {Y_test.shape}\")\n",
    "scaler= StandardScaler()\n",
    "x_train= scaler.fit_transform(x_train)\n",
    "x_test= scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 is  0.864783321688312\n",
      "mean_absolute_error 9.961057916168222\n",
      "mean_squared_error  145.51191974929068\n",
      "explained_variance_score 0.8648870835124843\n"
     ]
    }
   ],
   "source": [
    "random.seed(2000)\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(x_train, Y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "score= r2_score(Y_test, y_pred)\n",
    "print(\"r2 is \", score)\n",
    "h= mean_absolute_error(Y_test, y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(y_pred, Y_test))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))\n",
    "\n",
    "#f_statistic, p_value = f_regression(E1, Y)\n",
    "#selected_p_indices = np.where(p_value < 5e-04)[0]\n",
    "#selected_p_values = p_value[selected_p_indices]\n",
    "#print(\"Selected p-values:\")\n",
    "#print(selected_p_values)\n",
    "#print(\"Number of selected features:\", len(selected_p_indices))\n",
    "#selected_f_values = f_statistic[selected_p_indices]\n",
    "#print(\"Selected F-values:\")\n",
    "#print(selected_f_values)\n",
    "#top_f_indices = np.argsort(-selected_f_values)\n",
    "#print(\"Indices of selected features with top F-values:\")\n",
    "#print(top_f_indices)\n",
    "#top_f_features = E1.iloc[:, selected_p_indices[top_f_indices]]\n",
    "#print(\"Top selected features:\")\n",
    "#print(top_f_features)\n",
    "#Elasticnet_LR_sigSNP=E1.iloc[:, selected_p_indices]\n",
    "#Elasticnet_LR_sigSNP\n",
    "#Elasticnet_LR_sig_col= list(Elasticnet_LR_sigSNP.columns)\n",
    "#Elasticnet_LR_sig_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## r2_score_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 is  0.864783321688312\n"
     ]
    }
   ],
   "source": [
    "def r2_score_manual(y_true, y_pred):\n",
    "    # Convert inputs to NumPy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate the mean of the actual values\n",
    "    y_mean = np.mean(y_true)\n",
    "    \n",
    "    # Residual sum of squares\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    \n",
    "    # Total sum of squares\n",
    "    ss_tot = np.sum((y_true - y_mean) ** 2)\n",
    "    \n",
    "    # RÂ² score\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "\n",
    "r2_man = r2_score_manual(Y_test, y_pred)\n",
    "print(\"r2 is \",r2_man)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 is  0.04446024632645307\n",
      "r2 is  0.04446024632645307\n",
      "mean_absolute_error 26.018259740259737\n",
      "mean_squared_error  1028.2934449350648\n",
      "explained_variance_score 0.04871531452940969\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=9, min_samples_leaf=4, min_samples_split=2, \n",
    "                                     n_jobs=70,random_state=2000)\n",
    "\n",
    "# Create RFECV object\n",
    "rfecv = RFECV(estimator=rf_regressor, step=30, cv=5, scoring='neg_mean_squared_error', n_jobs=70)\n",
    "\n",
    "# Fit RFECV on the training data\n",
    "rfecv.fit(x_train, Y_train)\n",
    "\n",
    "# Manually set the number of features you want to keep\n",
    "desired_num_features =100\n",
    "# Manually set the number of features you want to keep\n",
    "\n",
    "\n",
    "# Get the selected features based on the desired number\n",
    "selected_features = rfecv.get_support(indices=True)[:desired_num_features]\n",
    "# Print the indices of the selected features\n",
    "#print(\"Selected features indices:\", selected_features)\n",
    "\n",
    "# Transform the training and testing data to keep only the selected features\n",
    "X_train_selected = rfecv.transform(x_train)[:, selected_features]\n",
    "X_test_selected = rfecv.transform(x_test)[:, selected_features]\n",
    "\n",
    "\n",
    "# Get the optimal number of features\n",
    "optimal_num_features = rfecv.n_features_\n",
    "\n",
    "# Print the optimal number of features\n",
    "#print(f\"Optimal number of features: {optimal_num_features}\")\n",
    "\n",
    "\n",
    "# Train a Random Forest Regressor on the selected features\n",
    "rf_regressor_selected = RandomForestRegressor(n_estimators=100, random_state=2000)\n",
    "rf_regressor_selected.fit(X_train_selected, Y_train)\n",
    "\n",
    "# Make predictions on the test set using the model with selected features\n",
    "y_pred= rf_regressor_selected.predict(X_test_selected)\n",
    "score= r2_score(Y_test, y_pred)\n",
    "print(\"r2 is \", score)\n",
    "r2_man = r2_score_manual(Y_test, y_pred)\n",
    "print(\"r2 is \",r2_man)\n",
    "h= mean_absolute_error(Y_test, y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))\n",
    "#Elasticnet_RF_sigSNP=E1.iloc[:,selected_features]\n",
    "#Elasticnet_RF_sig_col= list(Elasticnet_RF_sigSNP.columns)\n",
    "#Elasticnet_RF_sig_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.inspection import permutation_importance\n",
    "\n",
    "#svr = RandomForestRegressor(n_estimators=100, max_depth=9, min_samples_leaf=4, min_samples_split=2, n_jobs=70,random_state=2000)\n",
    "#svr.fit(x_train, Y_train)\n",
    "#y_pred= svr.predict(x_test)\n",
    "#score= r2_score(Y_test, y_pred)\n",
    "#print(\"r2 is \", score)\n",
    "#h= mean_absolute_error(Y_test, y_pred)\n",
    "#print(\"mean_absolute_error\", h)\n",
    "#print(\"mean_squared_error \", mean_squared_error(y_pred, Y_test))\n",
    "#print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))\n",
    "\n",
    "\n",
    "#perm_importance= permutation_importance(svr, x_test, Y_test, n_repeats=10, random_state=2000, n_jobs=70)\n",
    "\n",
    "#imp_score=perm_importance.importances_mean\n",
    "#feature_indices = np.argsort(np.abs(imp_score))[::-1]\n",
    "#print(f\"Selected feature indices are {feature_indices}\")\n",
    "# # Specify the number of features you want to select based on magnitude\n",
    "#num_selected_features = 100\n",
    "#top_100=feature_indices[:num_selected_features]\n",
    "#print(f\"Top 100 fearure indices are {feature_indices}\")\n",
    "#imp_score_100=imp_score[top_100]\n",
    "#print(f\"Top 100 fearures permutation scores are {imp_score_100}\")\n",
    "#Elasticnet_SVR_sigSNP= E1.iloc[:,top_100]\n",
    "#Elasticnet_SVR_sig_col= list(Elasticnet_SVR_sigSNP.columns)\n",
    "#Elasticnet_SVR_sig_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 is  0.8981861028949196\n",
      "r2 is  0.8981861028949196\n",
      "mean_absolute_error 8.666417701049134\n",
      "mean_squared_error  109.56588942945788\n",
      "explained_variance_score 0.8982207596651699\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "svr = SVR(kernel = 'sigmoid' , C=100, epsilon= 0.01, gamma= 'scale', cache_size=800)\n",
    "svr.fit(x_train, Y_train)\n",
    "y_pred= svr.predict(x_test)\n",
    "score= r2_score(Y_test, y_pred)\n",
    "print(\"r2 is \", score)\n",
    "r2_man = r2_score_manual(Y_test, y_pred)\n",
    "print(\"r2 is \",r2_man)\n",
    "h= mean_absolute_error(Y_test, y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))\n",
    "\n",
    "\n",
    "#perm_importance= permutation_importance(svr, x_test, Y_test, n_repeats=10, random_state=2000, n_jobs=70)\n",
    "\n",
    "#imp_score=perm_importance.importances_mean\n",
    "#feature_indices = np.argsort(np.abs(imp_score))[::-1]\n",
    "#print(f\"Selected feature indices are {feature_indices}\")\n",
    "# # Specify the number of features you want to select based on magnitude\n",
    "#num_selected_features = 100\n",
    "#top_100=feature_indices[:num_selected_features]\n",
    "#print(f\"Top 100 fearure indices are {feature_indices}\")\n",
    "#imp_score_100=imp_score[top_100]\n",
    "#print(f\"Top 100 fearures permutation scores are {imp_score_100}\")\n",
    "#Elasticnet_SVR_sigSNP= E1.iloc[:,top_100]\n",
    "#Elasticnet_SVR_sig_col= list(Elasticnet_SVR_sigSNP.columns)\n",
    "#Elasticnet_SVR_sig_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elasticnet_SVR_sigSNP\n",
    "#l= df.iloc[:,:6]\n",
    "#l\n",
    "#u= pd.concat([l, Elasticnet_SVR_sigSNP], axis=1)\n",
    "#l= \n",
    "# o= np.save(\"/home/Vaishnavi/ssss\", u)\n",
    "# i= pd.read_(\"/home/Vaishnavi/ssss\")\n",
    "# i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#u.to_csv(\"/home/Elasticnet_SVR_100_csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o= pd.read_csv(\"/home/Elasticnet_SVR_100_csv\")\n",
    "#o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_indices_abs = np.argsort(np.abs(imp_score))[::-1]\n",
    "#len(feature_indices_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_indices_pos = np.argsort(imp_score)[::-1]\n",
    "#feature_indices_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p=np.intersect1d(feature_indices_abs, feature_indices_pos)\n",
    "#len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_selected_features = 150\n",
    "#top_100_abs=feature_indices_abs[:num_selected_features]\n",
    "#print(f\"Top 100 feature indices are {feature_indices_abs}\")\n",
    "#top_100_pos=feature_indices_pos[:num_selected_features]\n",
    "#print(f\"Top 100 fearure indices are {feature_indices_pos}\")\n",
    "#imp_score_100_abs=imp_score[top_100_abs]\n",
    "#print(\"abs\",imp_score_100_abs)\n",
    "#imp_score_100_pos=imp_score[top_100_pos]\n",
    "#print(\"pos\",imp_score_100_pos)\n",
    "# print(f\"Top 100 fearures permutation scores are {imp_score_100}\")\n",
    "#p=np.intersect1d(imp_score_100_abs, imp_score_100_pos)\n",
    "#len(p)\n",
    "#Elasticnet_SVR_sigSNP_abs= E1.iloc[:,top_100_abs]\n",
    "#print(\"abs\", Elasticnet_SVR_sigSNP_abs)\n",
    "#Elasticnet_SVR_sigSNP_pos= E1.iloc[:,top_100_pos]\n",
    "#print(\"pos\", Elasticnet_SVR_sigSNP_pos)\n",
    "#p=np.intersect1d(Elasticnet_SVR_sigSNP_abs, Elasticnet_SVR_sigSNP_pos)\n",
    "#p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elasticnet_SVR_sigSNP_abs= E1.iloc[:,top_100_abs]\n",
    "#Elasticnet_SVR_sig_col_abs= list(Elasticnet_SVR_sigSNP_abs.columns)\n",
    "#Elasticnet_SVR_sig_col_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elasticnet_SVR_sigSNP_pos= E1.iloc[:,top_100_pos]\n",
    "#Elasticnet_SVR_sig_col_pos= list(Elasticnet_SVR_sigSNP_pos.columns)\n",
    "#Elasticnet_SVR_sig_col_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p=np.unique(Elasticnet_SVR_sig_col_pos,Elasticnet_SVR_sig_col_abs)\n",
    "#p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(imp_score_100_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(imp_score_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(imp_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score -0.1486632285535232\n",
      "r2 is  -0.1486632285535232\n",
      "mean_absolute_error 27.825146048409596\n",
      "mean_squared_error  1236.121117743754\n",
      "explained_variance_score -0.14587096507071218\n"
     ]
    }
   ],
   "source": [
    "regressor=xgb.XGBRegressor(learning_rate= 0.01, n_estimators=900,  max_depth=10, eval_metric='rmsle', random_state=2000)\n",
    "regressor.fit(x_train, Y_train)\n",
    "y_pred= regressor.predict(x_test)\n",
    "score= r2_score(Y_test,y_pred)\n",
    "print(\"r2 score\",score)\n",
    "r2_man = r2_score_manual(Y_test, y_pred)\n",
    "print(\"r2 is \",r2_man)\n",
    "h= mean_absolute_error(Y_test,y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> Mutual information <span style=\"color:green\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (1282, 69902)\n",
      "Shape of y (1282,)\n"
     ]
    }
   ],
   "source": [
    "#X is Independent feature and y is dependent feature\n",
    "X= df.iloc[:, 6:]\n",
    "print(\"Shape of X\", X.shape)\n",
    "y= df.PHENOTYPE\n",
    "print(\"Shape of y\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of mutal info 69902\n",
      "Maximum value of mutualinfo <bound method NDFrame._add_numeric_operations.<locals>.max of rs12613647    8.081041e-02\n",
      "rs6986075     7.524483e-02\n",
      "rs506973      7.250038e-02\n",
      "rs2048756     7.244632e-02\n",
      "rs7085631     7.020267e-02\n",
      "                  ...     \n",
      "rs356053      1.023246e-06\n",
      "rs12486193    6.066328e-07\n",
      "rs6443440     6.011830e-07\n",
      "rs4077566     3.887198e-07\n",
      "rs12117033    1.353719e-07\n",
      "Length: 34154, dtype: float64>\n",
      "Length of mutal info non zero 34154\n",
      "print shape of selected snps by Mutual_info (1282, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the Mutual_info model\n",
    "mutual_info= mutual_info_regression(X, y, random_state=2000)\n",
    "print(\"Length of mutal info\", len(mutual_info))\n",
    "mutual_info= pd.Series(mutual_info, index=df.columns[6:])\n",
    "\n",
    "#Sort mutual_info values descending order\n",
    "vdesc= mutual_info.sort_values(ascending=False)\n",
    "vdesc_nonzero= vdesc[vdesc!=0]\n",
    "print(\"Maximum value of mutualinfo\", vdesc_nonzero.max)\n",
    "print(\"Length of mutal info non zero\", len(vdesc_nonzero))\n",
    "k= np.where(vdesc_nonzero)\n",
    "first_col= k[0]\n",
    "\n",
    "#Selected 5000 SNPs\n",
    "top5000 = 5000\n",
    "mi_pos= first_col[:top5000]\n",
    "\n",
    "#Final SNPs\n",
    "M1= X.iloc[:,mi_pos]\n",
    "print(\"print shape of selected snps by Mutual_info\",M1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape (1282,)\n",
      "x_train=(897, 5000), x_test=(385, 5000), Y_train= (897,), Y_test= (385,)\n"
     ]
    }
   ],
   "source": [
    "Y= df.PHENOTYPE.values\n",
    "print(\"Y.shape\",Y.shape)\n",
    "x_train, x_test, Y_train, Y_test= train_test_split(M1, Y, test_size=0.3, random_state=2000)\n",
    "print(f\"x_train={x_train.shape}, x_test={x_test.shape}, Y_train= {Y_train.shape}, Y_test= {Y_test.shape}\")\n",
    "scaler= StandardScaler()\n",
    "x_train= scaler.fit_transform(x_train)\n",
    "x_test= scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 is  -0.3636281264776131\n",
      "r2 is  -0.3636281264776131\n",
      "mean_absolute_error 31.257174085961733\n",
      "mean_squared_error  1467.4531942760675\n",
      "explained_variance_score -0.3617295943021641\n"
     ]
    }
   ],
   "source": [
    "random.seed(2000)\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(x_train, Y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "score= r2_score(Y_test,y_pred)\n",
    "print(\"r2 is \", score)\n",
    "r2_man = r2_score_manual(Y_test, y_pred)\n",
    "print(\"r2 is \",r2_man)\n",
    "h= mean_absolute_error(Y_test,y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))\n",
    "\n",
    "#f_statistic, p_value = f_regression(M1, Y)\n",
    "#selected_p_indices = np.where(p_value < 5e-04)[0]\n",
    "#selected_p_values = p_value[selected_p_indices]\n",
    "#print(\"Selected p-values:\")\n",
    "#print(selected_p_values)\n",
    "#print(\"Number of selected features:\", len(selected_p_indices))\n",
    "#selected_f_values = f_statistic[selected_p_indices]\n",
    "#print(\"Selected F-values:\")\n",
    "#print(selected_f_values)\n",
    "#top_f_indices = np.argsort(-selected_f_values)\n",
    "#print(\"Indices of selected features with top F-values:\")\n",
    "#print(top_f_indices)\n",
    "#top_f_features = M1.iloc[:, selected_p_indices[top_f_indices]]\n",
    "#print(\"Top selected features:\")\n",
    "#print(top_f_features)\n",
    "#MI_LR_sigSNP=M1.iloc[:, selected_p_indices]\n",
    "#MI_LR_sigSNP\n",
    "#MI_LR_sig_col= list(MI_LR_sigSNP.columns)\n",
    "#MI_LR_sig_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features indices: [False False False ... False False False]\n",
      "r2 is  0.013833386852806329\n",
      "r2 is  0.013833386852806329\n",
      "mean_absolute_error 26.451719178162453\n",
      "mean_squared_error  1061.2521980529993\n",
      "explained_variance_score 0.015694003645515986\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=9, min_samples_leaf=4, min_samples_split=2, \n",
    "                                     n_jobs=70,random_state=2000)\n",
    "\n",
    "# Create RFECV object\n",
    "rfecv = RFECV(estimator=rf_regressor, step=30, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit RFECV on the training data\n",
    "rfecv.fit(x_train, Y_train)\n",
    "\n",
    "# Manually set the number of features you want to keep\n",
    "desired_num_features =100\n",
    "# Manually set the number of features you want to keep\n",
    "\n",
    "\n",
    "# Get the selected features based on the desired number\n",
    "selected_features = rfecv.support_\n",
    "# Print the indices of the selected features\n",
    "print(\"Selected features indices:\", selected_features)\n",
    "\n",
    "# Transform the training and testing data to keep only the selected features\n",
    "X_train_selected = rfecv.transform(x_train)\n",
    "X_test_selected = rfecv.transform(x_test)\n",
    "\n",
    "\n",
    "# Get the optimal number of features\n",
    "optimal_num_features = rfecv.n_features_\n",
    "\n",
    "# Print the optimal number of features\n",
    "#print(f\"Optimal number of features: {optimal_num_features}\")\n",
    "\n",
    "\n",
    "# Train a Random Forest Regressor on the selected features\n",
    "rf_regressor_selected = RandomForestRegressor(n_estimators=100, random_state=2000)\n",
    "rf_regressor_selected.fit(X_train_selected, Y_train)\n",
    "\n",
    "# Make predictions on the test set using the model with selected features\n",
    "y_pred= rf_regressor_selected.predict(X_test_selected)\n",
    "\n",
    "score= r2_score(Y_test,y_pred)\n",
    "print(\"r2 is \", score)\n",
    "r2_man = r2_score_manual(Y_test, y_pred)\n",
    "print(\"r2 is \",r2_man)\n",
    "h= mean_absolute_error(Y_test,y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))\n",
    "#MI_RF_sigSNP=M1.iloc[:,selected_features]\n",
    "#MI_RF_sig_col= list(MI_RF_sigSNP.columns)\n",
    "#MI_RF_sig_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE is 32.07399894842477\n",
      "r2 is  -0.4511933589703607\n",
      "r2 is  -0.4511933589703607\n",
      "mean_absolute_error 32.07399894842477\n",
      "mean_squared_error  1561.6855422556678\n",
      "explained_variance_score -0.45035301832945995\n"
     ]
    }
   ],
   "source": [
    "svr = SVR(kernel = 'sigmoid' , C=100, epsilon= 0.01, gamma= 'scale')\n",
    "svr.fit(x_train, Y_train)\n",
    "y_pred= svr.predict(x_test)\n",
    "mae= print(f\"MAE is {mean_absolute_error(Y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "#perm_importance= permutation_importance(svr, x_test, Y_test, n_repeats=10, random_state=2000, n_jobs=70)\n",
    "\n",
    "#imp_score=perm_importance.importances_mean\n",
    "#feature_indices = np.argsort(np.abs(imp_score))[::-1]\n",
    "#print(f\"Selected feature indices are {feature_indices}\")\n",
    "\n",
    "# # Specify the number of features you want to select based on magnitude\n",
    "#num_selected_features = 100\n",
    "#top_100=feature_indices[:num_selected_features]\n",
    "#print(f\"Top 100 fearure indices are {feature_indices}\")\n",
    "#imp_score_100=imp_score[top_100]\n",
    "#print(f\"Top 100 fearures permutation scores are {imp_score_100}\")\n",
    "score= r2_score(Y_test,y_pred)\n",
    "print(\"r2 is \", score)\n",
    "r2_man = r2_score_manual(Y_test, y_pred)\n",
    "print(\"r2 is \",r2_man)\n",
    "h= mean_absolute_error(Y_test,y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))\n",
    "#MI_SVR_sigSNP= M1.iloc[:,top_100]\n",
    "#MI_SVR_sig_col= list(MI_SVR_sigSNP.columns)\n",
    "#MI_SVR_sig_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost for mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score -0.20577805816084016\n",
      "r2 is  -0.20577805816084016\n",
      "mean_absolute_error 28.32920542134867\n",
      "mean_squared_error  1297.584604394098\n",
      "explained_variance_score -0.20260762111723296\n"
     ]
    }
   ],
   "source": [
    "## XGBoost for mutual information\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "regressor=xgb.XGBRegressor(learning_rate= 0.01, n_estimators=900,  max_depth=10, eval_metric='rmsle', random_state=2000)\n",
    "regressor.fit(x_train, Y_train)\n",
    "y_pred= regressor.predict(x_test)\n",
    "score= r2_score(Y_test,y_pred)\n",
    "print(\"r2 score\",score)\n",
    "r2_man = r2_score_manual(Y_test, y_pred)\n",
    "print(\"r2 is \",r2_man)\n",
    "h= mean_absolute_error(Y_test, y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting for Elastic-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "reg = GradientBoostingRegressor(loss='squared_error',\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=250,\n",
    "    subsample=1.0,\n",
    "    criterion='squared_error',\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_depth=15,\n",
    "    min_impurity_decrease=0.0,\n",
    "    init=None,\n",
    "    max_features=None,\n",
    "    alpha=0.9,\n",
    "    verbose=0,\n",
    "    max_leaf_nodes=None,\n",
    "    warm_start=False,\n",
    "    validation_fraction=0.2,\n",
    "    n_iter_no_change=None,\n",
    "    tol=0.0001,\n",
    "    ccp_alpha=0.0,random_state=2000)\n",
    "reg.fit(x_train, Y_train)\n",
    "y_pred=reg.predict(x_test)\n",
    "score= r2_score(Y_test,y_pred)\n",
    "print(\"r2 score\",score)\n",
    "h= mean_absolute_error(Y_test,y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost for elastic-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=xgb.XGBRegressor(learning_rate= 0.01, n_estimators=900,  max_depth=10, eval_metric='rmsle', random_state=2000)\n",
    "regressor.fit(x_train, Y_train)\n",
    "y_pred= regressor.predict(x_test)\n",
    "score= r2_score(y_pred, Y_test)\n",
    "print(\"r2 score\",score)\n",
    "h= mean_absolute_error(Y_test,y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost for lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=xgb.XGBRegressor(learning_rate= 0.01, n_estimators=900,  max_depth=10, eval_metric='rmsle', random_state=2000)\n",
    "regressor.fit(x_train, Y_train)\n",
    "y_pred= regressor.predict(x_test)\n",
    "score= r2_score(Y_test,y_pred)\n",
    "print(\"r2 score\",score)\n",
    "h= mean_absolute_error(Y_test,y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost for ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=xgb.XGBRegressor(learning_rate= 0.01, n_estimators=900,  max_depth=10, eval_metric='rmsle', random_state=2000)\n",
    "regressor.fit(x_train, Y_train)\n",
    "y_pred= regressor.predict(x_test)\n",
    "score= r2_score(Y_test,y_pred)\n",
    "print(\"r2 score\",score)\n",
    "h= mean_absolute_error(Y_test,y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost for mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regressor=xgb.XGBRegressor(learning_rate= 0.01, n_estimators=900,  max_depth=10, eval_metric='rmsle', random_state=2000)\n",
    "regressor.fit(x_train, Y_train)\n",
    "y_pred= regressor.predict(x_test)\n",
    "score= r2_score(Y_test,y_pred)\n",
    "print(\"r2 score\",score)\n",
    "h= mean_absolute_error(Y_test, y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(Y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=xgb.XGBRegressor(learning_rate= 0.01, n_estimators=1000,  max_depth=15, eval_metric='rmsle', random_state=2000,\n",
    "                          n_jobs=10)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred= regressor.predict(X_test)\n",
    "score= r2_score(Y_test,y_pred)\n",
    "print(\"r2 score\",score)\n",
    "h= mean_absolute_error(Y_test,y_pred)\n",
    "print(\"mean_absolute_error\", h)\n",
    "print(\"mean_squared_error \", mean_squared_error(y_test, y_pred))\n",
    "print(\"explained_variance_score\", explained_variance_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> SNP Enrichment <span style=\"color:green\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_snp = pybiomart.Dataset('hsapiens_snp', host='http://www.ensembl.org')\n",
    "db_snp.list_attributes().head(5)\n",
    "#db_snp.list_filters()\n",
    "snp_results=[]\n",
    "for i in Elasticnet_SVR_sig_col:\n",
    "    snp_ens = db_snp.query(attributes=['refsnp_id','refsnp_source_description','chr_name','chrom_start', 'phenotype_name',\n",
    "                                     'consequence_type_tv', 'minor_allele', 'clinical_significance',\n",
    "                                   'associated_gene',  'ensembl_gene_stable_id'],filters={'snp_filter':[i]})\n",
    "    snp_results.append(snp_ens)\n",
    "   \n",
    "combined_df = pd.concat(snp_results, ignore_index=True)\n",
    "output_csv_path = '/home/Vaishnavi/E1_SVR_Ensembl.csv'  \n",
    "combined_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Saved combined data to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pybiomart.Dataset(name='hsapiens_gene_ensembl', host='http://www.ensembl.org')\n",
    "l=[]\n",
    "p=combined_df.iloc[:,9]\n",
    "for i in p:\n",
    "    \n",
    "    o=dataset.query(attributes=['ensembl_gene_id', 'entrezgene_id','chromosome_name','description','hgnc_symbol','phenotype_description'],\n",
    "             filters={'link_ensembl_gene_id': [i]})\n",
    "    l.append(o)\n",
    "    \n",
    "gene_info= pd.concat(l)\n",
    "print(gene_info)\n",
    "output_csv_path = '/home/Vaishnavi/E1_SVR_phenoinfo.csv'  \n",
    "gene_info.to_csv(output_csv_path, index=False)\n",
    "print(f\"Saved combined data to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file \n",
    "df= pd.read_table(\"/home/Vaishnavi/ldlGWAS/GWAStutorialldlraw.raw\", delim_whitespace= True)\n",
    "print(df.head())\n",
    "print(\"Shape of file\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_names= list(df.columns[:6])\n",
    "new_col_names.extend([col_name[:-2] for col_name in df.columns[6:]])\n",
    "df.columns=new_col_names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df.iloc[:,6:]\n",
    "y= df.PHENOTYPE\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=2000)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                  y_train,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=69902, activation='relu'))\n",
    "model.add(Dense(10, activation='linear'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam',\n",
    "              metrics=['mean_absolute_error'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    batch_size=30, validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
